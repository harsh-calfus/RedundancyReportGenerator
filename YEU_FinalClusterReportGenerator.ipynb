{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c051b987",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import json\n",
    "from collections import defaultdict\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "import xlsxwriter\n",
    "import numpy as np\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5f6d94c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "address_similarity_threshold = 75\n",
    "\n",
    "classification_path = \"content/CustomerClusterReport.xlsx\"\n",
    "site_path = \"content/MILGARD DATA 26-07-25 DEV2 (1).xlsx\"\n",
    "site_sheet_name = \"Site and Site Use\"\n",
    "output_path = \"content/FinalReport.xlsx\"\n",
    "address_cluster_path = \"content/AddressClusterReport.xlsx\"\n",
    "\n",
    "classification_df = pd.read_excel(classification_path)\n",
    "df_sites = pd.read_excel(site_path, site_sheet_name)\n",
    "\n",
    "report_cols = [\n",
    "    'Site ID', 'Site Name', 'Site Purpose',\n",
    "    'Address Line 1', 'Address Line 2', 'Mail Stop', 'City', 'State/Province',\n",
    "    'Postal Code', 'County', 'Country', 'Party ID'\n",
    "]\n",
    "\n",
    "address_cols = [\n",
    "    'Address Line 1', 'Address Line 2', 'Mail Stop', 'City',\n",
    "    'State/Province', 'Postal Code', 'County', 'Country'\n",
    "]\n",
    "\n",
    "columns_to_keep = [\n",
    "    'classification', 'Party Name', 'Party ID',\n",
    "    'Site ID', 'Address Line 1', 'Address Line 2', 'City',\n",
    "    'State/Province', 'Postal Code', 'County', 'Country'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d721174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sites = df_sites[report_cols]\n",
    "merged_df = classification_df.merge(df_sites, on='Party ID', how='inner').fillna(\"\")\n",
    "\n",
    "def normalize_text(s):\n",
    "    return re.sub(r\"\\s+\", \" \", re.sub(r\"[^\\w\\s]\", \" \", str(s).lower())).strip()\n",
    "\n",
    "def build_weighted_text(row):\n",
    "    parts = []\n",
    "    for col in address_cols:\n",
    "        val = str(row.get(col, '')).strip()\n",
    "        if col == \"Postal Code\" and \"-\" in val:\n",
    "            val = val.split(\"-\")[0].strip()\n",
    "        parts.append((val + \" \") * 3 if col in [\"Address Line 1\", \"Postal Code\"] else val)\n",
    "    return normalize_text(\" \".join(parts))\n",
    "\n",
    "merged_df[\"full_address\"] = merged_df.apply(build_weighted_text, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1348b385",
   "metadata": {},
   "outputs": [],
   "source": [
    "sbert = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embs = sbert.encode(merged_df[\"full_address\"].tolist(), convert_to_tensor=False, normalize_embeddings=True)\n",
    "\n",
    "merged_df[\"row_id\"] = merged_df.index\n",
    "party_site_map = defaultdict(list)\n",
    "for idx, row in merged_df.iterrows():\n",
    "    party_site_map[row[\"Party ID\"]].append(row[\"row_id\"])\n",
    "\n",
    "similarity_column = []\n",
    "for idx1, row1 in merged_df.iterrows():\n",
    "    pid1, emb1 = row1[\"Party ID\"], embs[row1[\"row_id\"]].reshape(1, -1)\n",
    "    sims = []\n",
    "    for pid2, indices in party_site_map.items():\n",
    "        if pid1 == pid2:\n",
    "            continue\n",
    "        for idx2 in indices:\n",
    "            row2 = merged_df.loc[idx2]\n",
    "            emb2 = embs[idx2].reshape(1, -1)\n",
    "            sim = float(cosine_similarity(emb1, emb2)[0][0])\n",
    "            sims.append({\"To Party\": int(pid2), \"To Site\": str(row2[\"Site ID\"]), \"Similarity\": f\"{sim * 100:.2f}%\"})\n",
    "    similarity_column.append(json.dumps(sims, ensure_ascii=False))\n",
    "\n",
    "merged_df[\"Similarity to Other Parties\"] = similarity_column\n",
    "merged_df[\"Similarity Parsed\"] = merged_df[\"Similarity to Other Parties\"].apply(json.loads)\n",
    "\n",
    "merged_df.to_excel(\"content/merged_Cluster.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c2624982",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_clusters = []\n",
    "\n",
    "for class_id, group in merged_df.groupby(\"classification\"):\n",
    "    pids_in_class = set(group[\"Party ID\"])\n",
    "\n",
    "    qualified_rows = []\n",
    "    for _, row in group.iterrows():\n",
    "        sims = row[\"Similarity Parsed\"]\n",
    "        for entry in sims:\n",
    "            if float(entry[\"Similarity\"].replace('%', '')) > address_similarity_threshold and entry[\"To Party\"] in pids_in_class:\n",
    "                qualified_rows.append(row)\n",
    "                break\n",
    "\n",
    "    if len(qualified_rows) >= 2:\n",
    "        final_clusters.append(pd.DataFrame(qualified_rows))\n",
    "\n",
    "if final_clusters:\n",
    "    result_df = pd.concat(final_clusters).copy()\n",
    "    result_df.sort_values(by=[\"classification\", \"Party ID\"], inplace=True)\n",
    "    old_to_new_class = {old: new for new, old in enumerate(sorted(result_df[\"classification\"].unique()), start=1)}\n",
    "    result_df[\"classification\"] = result_df[\"classification\"].map(old_to_new_class)\n",
    "else:\n",
    "    result_df = pd.DataFrame(columns=merged_df.columns)\n",
    "\n",
    "type_labels_map = {}\n",
    "\n",
    "for class_id, group in result_df.groupby(\"classification\"):\n",
    "    is_pure_customer_duplication = True\n",
    "\n",
    "    for sims in group[\"Similarity to Other Parties\"]:\n",
    "        parsed = json.loads(sims)\n",
    "        sims_floats = [float(entry[\"Similarity\"].replace('%', '')) for entry in parsed]\n",
    "\n",
    "        if 100.0 not in sims_floats:\n",
    "            is_pure_customer_duplication = False\n",
    "            break\n",
    "\n",
    "        if any(sim != 100.0 for sim in sims_floats):\n",
    "            is_pure_customer_duplication = False\n",
    "            break\n",
    "\n",
    "    label = \"Customer Duplication\" if is_pure_customer_duplication else \"Customer + Address Duplication\"\n",
    "    type_labels_map[class_id] = label\n",
    "\n",
    "result_df[\"type\"] = result_df[\"classification\"].map(type_labels_map)\n",
    "result_df = result_df[[\"type\"] + columns_to_keep]\n",
    "\n",
    "result_df.drop(columns=[\"Similarity Parsed\", \"full_address\", \"row_id\"], errors='ignore', inplace=True)\n",
    "result_df = result_df[[\"type\"] + columns_to_keep]\n",
    "\n",
    "addr_dup_df = pd.read_excel(address_cluster_path)\n",
    "addr_dup_df[\"type\"] = \"Address Duplication\"\n",
    "final_df = pd.concat([result_df, addr_dup_df], ignore_index=True)\n",
    "\n",
    "final_df = final_df.replace([np.inf, -np.inf], np.nan)\n",
    "final_df = final_df.fillna(\"\")\n",
    "\n",
    "final_df = final_df[[\"type\"] + columns_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2c14f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "site_df = pd.read_excel(site_path, sheet_name=\"Site and Site Use\")\n",
    "site_account_df = site_df[[\"Site ID\", \"Account Number\"]]\n",
    "\n",
    "# Merge Account Number into final_df using Site ID\n",
    "final_df = final_df.merge(site_account_df, on=\"Site ID\", how=\"left\")\n",
    "\n",
    "# Move Account Number column after Site ID (optional)\n",
    "cols = final_df.columns.tolist()\n",
    "if \"Account Number\" in cols and \"Site ID\" in cols:\n",
    "    site_id_idx = cols.index(\"Site ID\")\n",
    "    # Remove Account Number and insert after Site ID\n",
    "    cols.remove(\"Account Number\")\n",
    "    cols.insert(site_id_idx + 1, \"Account Number\")\n",
    "    final_df = final_df[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f12e8a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'content/FinalReport.xlsx'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with pd.ExcelWriter(output_path, engine='xlsxwriter') as writer:\n",
    "    final_df.to_excel(writer, index=False, sheet_name='Filtered')\n",
    "    \n",
    "    workbook = writer.book\n",
    "    worksheet = writer.sheets['Filtered']\n",
    "\n",
    "    header_format = workbook.add_format({\n",
    "        'bold': True,\n",
    "        'bg_color': '#D3D3D3',\n",
    "        'border': 1\n",
    "    })\n",
    "\n",
    "    border_format = workbook.add_format({'border': 1})\n",
    "    highlight_format = workbook.add_format({'bg_color': '#FFFACD', 'border': 1})\n",
    "\n",
    "    for col_num, column_name in enumerate(final_df.columns):\n",
    "        worksheet.write(0, col_num, column_name, header_format)\n",
    "\n",
    "    worksheet.autofilter(0, 0, 0, len(final_df.columns) - 1)\n",
    "\n",
    "    max_row = final_df.shape[0]\n",
    "    max_col = final_df.shape[1]\n",
    "\n",
    "    address_col_indices = [final_df.columns.get_loc(col) for col in address_cols if col in final_df.columns]\n",
    "\n",
    "    for class_id, group in final_df.groupby('classification'):\n",
    "        cluster_indices = group.index.tolist()\n",
    "        for col_idx in address_col_indices:\n",
    "            col_name = final_df.columns[col_idx]\n",
    "            mode_val = group[col_name].mode().iloc[0] if not group[col_name].mode().empty else None\n",
    "            highlight_entire_col = any(\n",
    "                mode_val is not None and cell_val != mode_val\n",
    "                for cell_val in group[col_name]\n",
    "            )\n",
    "            for row_idx in cluster_indices:\n",
    "                excel_row = row_idx + 1\n",
    "                cell_val = final_df.iloc[row_idx, col_idx]\n",
    "                if highlight_entire_col:\n",
    "                    worksheet.write(excel_row, col_idx, cell_val, highlight_format)\n",
    "                else:\n",
    "                    worksheet.write(excel_row, col_idx, cell_val, border_format)\n",
    "        for row_idx in cluster_indices:\n",
    "            for col in range(max_col):\n",
    "                if col not in address_col_indices:\n",
    "                    excel_row = row_idx + 1\n",
    "                    worksheet.write(excel_row, col, final_df.iloc[row_idx, col], border_format)\n",
    "\n",
    "    for idx, col in enumerate(final_df.columns):\n",
    "        col_data = final_df[col].astype(str)\n",
    "        max_len = max([len(str(col))] + col_data.map(len).tolist())\n",
    "        worksheet.set_column(idx, idx, max_len + 2)\n",
    "\n",
    "output_path"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
